{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化摄像头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for jetbot: \n"
     ]
    }
   ],
   "source": [
    "!echo 'jetbot' | sudo -S systemctl restart nvargus-daemon && printf '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetcam.csi_camera import CSICamera\n",
    "# from jetcam.usb_camera import USBCamera\n",
    "\n",
    "camera = CSICamera(width=640, height=360)\n",
    "# camera = USBCamera(width=224, height=224)\n",
    "#camera.running = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import traitlets\n",
    "from IPython.display import display\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import platform\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "image = camera.read()\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_widget = ipywidgets.Image(format='jpeg')\n",
    "\n",
    "image_widget.value = bgr8_to_jpeg(image)\n",
    "\n",
    "#display(image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our list of known face encodings and a matching list of metadata about each face.\n",
    "known_face_encodings = []\n",
    "known_face_metadata = []\n",
    "\n",
    "\n",
    "def save_known_faces():\n",
    "    with open(\"known_faces.dat\", \"wb\") as face_data_file:\n",
    "        face_data = [known_face_encodings, known_face_metadata]\n",
    "        pickle.dump(face_data, face_data_file)\n",
    "        print(\"Known faces backed up to disk.\")\n",
    "\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_metadata\n",
    "\n",
    "    try:\n",
    "        with open(\"known_faces.dat\", \"rb\") as face_data_file:\n",
    "            known_face_encodings, known_face_metadata = pickle.load(face_data_file)\n",
    "            print(\"Known faces loaded from disk.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"No previous face data found - starting with a blank known face list.\")\n",
    "        pass\n",
    "\n",
    "\n",
    "def running_on_jetson_nano():\n",
    "    # To make the same code work on a laptop or on a Jetson Nano, we'll detect when we are running on the Nano\n",
    "    # so that we can access the camera correctly in that case.\n",
    "    # On a normal Intel laptop, platform.machine() will be \"x86_64\" instead of \"aarch64\"\n",
    "    return platform.machine() == \"aarch64\"\n",
    "\n",
    "\n",
    "def get_jetson_gstreamer_source(capture_width=1280, capture_height=720, display_width=1280, display_height=720, framerate=60, flip_method=0):\n",
    "    \"\"\"\n",
    "    Return an OpenCV-compatible video source description that uses gstreamer to capture video from the camera on a Jetson Nano\n",
    "    \"\"\"\n",
    "    return (\n",
    "            f'nvarguscamerasrc ! video/x-raw(memory:NVMM), ' +\n",
    "            f'width=(int){capture_width}, height=(int){capture_height}, ' +\n",
    "            f'format=(string)NV12, framerate=(fraction){framerate}/1 ! ' +\n",
    "            f'nvvidconv flip-method={flip_method} ! ' +\n",
    "            f'video/x-raw, width=(int){display_width}, height=(int){display_height}, format=(string)BGRx ! ' +\n",
    "            'videoconvert ! video/x-raw, format=(string)BGR ! appsink'\n",
    "            )\n",
    "\n",
    "\n",
    "def register_new_face(face_encoding, face_image):\n",
    "    \"\"\"\n",
    "    Add a new person to our list of known faces\n",
    "    \"\"\"\n",
    "    # Add the face encoding to the list of known faces\n",
    "    known_face_encodings.append(face_encoding)\n",
    "    # Add a matching dictionary entry to our metadata list.\n",
    "    # We can use this to keep track of how many times a person has visited, when we last saw them, etc.\n",
    "    known_face_metadata.append({\n",
    "        \"first_seen\": datetime.now(),\n",
    "        \"first_seen_this_interaction\": datetime.now(),\n",
    "        \"last_seen\": datetime.now(),\n",
    "        \"seen_count\": 1,\n",
    "        \"seen_frames\": 1,\n",
    "        \"face_image\": face_image,\n",
    "    })\n",
    "\n",
    "\n",
    "def lookup_known_face(face_encoding):\n",
    "    \"\"\"\n",
    "    See if this is a face we already have in our face list\n",
    "    \"\"\"\n",
    "    metadata = None\n",
    "\n",
    "    # If our known face list is empty, just return nothing since we can't possibly have seen this face.\n",
    "    if len(known_face_encodings) == 0:\n",
    "        return metadata\n",
    "\n",
    "    # Calculate the face distance between the unknown face and every face on in our known face list\n",
    "    # This will return a floating point number between 0.0 and 1.0 for each known face. The smaller the number,\n",
    "    # the more similar that face was to the unknown face.\n",
    "    face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "\n",
    "    # Get the known face that had the lowest distance (i.e. most similar) from the unknown face.\n",
    "    best_match_index = np.argmin(face_distances)\n",
    "\n",
    "    # If the face with the lowest distance had a distance under 0.6, we consider it a face match.\n",
    "    # 0.6 comes from how the face recognition model was trained. It was trained to make sure pictures\n",
    "    # of the same person always were less than 0.6 away from each other.\n",
    "    # Here, we are loosening the threshold a little bit to 0.65 because it is unlikely that two very similar\n",
    "    # people will come up to the door at the same time.\n",
    "    if face_distances[best_match_index] < 0.65:\n",
    "        # If we have a match, look up the metadata we've saved for it (like the first time we saw it, etc)\n",
    "        metadata = known_face_metadata[best_match_index]\n",
    "\n",
    "        # Update the metadata for the face so we can keep track of how recently we have seen this face.\n",
    "        metadata[\"last_seen\"] = datetime.now()\n",
    "        metadata[\"seen_frames\"] += 1\n",
    "\n",
    "        # We'll also keep a total \"seen count\" that tracks how many times this person has come to the door.\n",
    "        # But we can say that if we have seen this person within the last 5 minutes, it is still the same\n",
    "        # visit, not a new visit. But if they go away for awhile and come back, that is a new visit.\n",
    "        if datetime.now() - metadata[\"first_seen_this_interaction\"] > timedelta(minutes=5):\n",
    "            metadata[\"first_seen_this_interaction\"] = datetime.now()\n",
    "            metadata[\"seen_count\"] += 1\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def update_image(change):\n",
    "    frame = change['new']\n",
    "    global number_of_faces_since_save\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "    # Find all the face locations and face encodings in the current frame of video\n",
    "    face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "    # Loop through each detected face and see if it is one we have seen before\n",
    "    # If so, we'll give it a label that we'll draw on top of the video.\n",
    "    face_labels = []\n",
    "    for face_location, face_encoding in zip(face_locations, face_encodings):\n",
    "        # See if this face is in our list of known faces.\n",
    "        metadata = lookup_known_face(face_encoding)\n",
    "\n",
    "        # If we found the face, label the face with some useful information.\n",
    "        if metadata is not None:\n",
    "            time_at_door = datetime.now() - metadata['first_seen_this_interaction']\n",
    "            face_label = f\"At door {int(time_at_door.total_seconds())}s\"\n",
    "            #print(\"at door\")\n",
    "\n",
    "        # If this is a brand new face, add it to our list of known faces\n",
    "        else:\n",
    "            face_label = \"New visitor!\"\n",
    "\n",
    "            # Grab the image of the the face from the current frame of video\n",
    "            top, right, bottom, left = face_location\n",
    "            face_image = small_frame[top:bottom, left:right]\n",
    "            face_image = cv2.resize(face_image, (150, 150))\n",
    "            #print(\"new visitor\")\n",
    "\n",
    "            # Add the new face to our known face data\n",
    "            register_new_face(face_encoding, face_image)\n",
    "\n",
    "        face_labels.append(face_label)\n",
    "\n",
    "    # Draw a box around each face and label each face\n",
    "    for (top, right, bottom, left), face_label in zip(face_locations, face_labels):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        frame=cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        frame=cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        frame=cv2.putText(frame, face_label, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
    "\n",
    "    # Display recent visitor images\n",
    "    number_of_recent_visitors = 0\n",
    "    for metadata in known_face_metadata:\n",
    "        # If we have seen this person in the last minute, draw their image\n",
    "        if datetime.now() - metadata[\"last_seen\"] < timedelta(seconds=10) and metadata[\"seen_frames\"] > 5:\n",
    "            # Draw the known face image\n",
    "            x_position = number_of_recent_visitors * 150\n",
    "            frame[30:180, x_position:x_position + 150] = metadata[\"face_image\"]\n",
    "            number_of_recent_visitors += 1\n",
    "\n",
    "            # Label the image with how many times they have visited\n",
    "            visits = metadata['seen_count']\n",
    "            visit_label = f\"{visits} visits\"\n",
    "            if visits == 1:\n",
    "                visit_label = \"First visit\"\n",
    "            frame=cv2.putText(frame, visit_label, (x_position + 10, 170), cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "    if number_of_recent_visitors > 0:\n",
    "        frame=cv2.putText(frame, \"Visitors at Door\", (5, 18), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
    "\n",
    "\n",
    "\n",
    "    # Display the final frame of video with boxes drawn around each detected fames\n",
    "    #cv2.imshow('Video', frame)\n",
    "    #frame=cv2.circle(frame, (100, 100), 8, (255, 0, 0), 3)\n",
    "    image_widget.value = bgr8_to_jpeg(frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    \n",
    "    '''\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        save_known_faces()\n",
    "        break\n",
    "    \n",
    "    '''\n",
    "        \n",
    "    # We need to save our known faces back to disk every so often in case something crashes.\n",
    "    if len(face_locations) > 0 and number_of_faces_since_save > 100:\n",
    "        save_known_faces()\n",
    "        number_of_faces_since_save = 0\n",
    "    else:\n",
    "        number_of_faces_since_save += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.running = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "global number_of_faces_since_save\n",
    "number_of_faces_since_save = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_faces_since_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known faces loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "load_known_faces()    \n",
    "camera.observe(update_image, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b73fcf569c644f4b6211cf988188767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known faces backed up to disk.\n",
      "Known faces backed up to disk.\n",
      "Known faces backed up to disk.\n"
     ]
    }
   ],
   "source": [
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unobserve all callbacks from camera in case we are running this cell for second time\n",
    "camera.unobserve_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
